Start : 16October2024

Tut will have us set up a NodeJS and Express App inside a docker container with a MongoDB and Redis
npm init -y #will create a 'package.json' in your cwd
npm install express #will install express, add it to 'package.json' 'package-lock.json'
create a 'index.js'
const express = require("express"); #imports express
const app = express();
app.get("/", (req, res) => {res.send("<h1>Test Success</h2>");}); #quick route for testing purposes, "/" from route path, will send back the response as an h2 element 'Test Success'
const port = process.env.PORT || 3000; #will set the port to the environment variable, or default to 3000
app.listen(port, () => console.log(`listening on port${port}`)); #when appconnects to a port will print out which port it is on
#above is the simplest express app you can build
'node index.js' #in the terminal, will run the above script on 'localhost:3000'

Docker: 
Install Docker on Arch Linux: 
https://docs.docker.com/desktop/install/linux/archlinux/
https://wiki.archlinux.org/title/Docker
-Couldn't install docker-desktop, did a 'sudo pacman -Syu' and going to reboot, system update and reboot worked, now have docker desktop installed on Arch linux(btw.)
https://hub.docker.com/ #where images are saved
https://hub.docker.com/_/node #official node docker image, a lightweight img with node already installed, you can select which version you want
The image above doesn't have everything we want, the idea(l) of an image is that it has everything in it for your application to work, so you will need:
Source code(Application)/Dependancies/etc
So we are going to create our own custom image based on that node image, I need to fix my '.gitignore' file so I'm not uploading the docker image to github
Take that node image, copy our source code into that node image, then install all dependancies, then build that final image that has everything we need to run the app
To create a custom image, you need to create a 'Dockerfile' which is just a set of instructions that docker is going to run to create a custom image
  It will run a set of commands the create the docker image :
The first command you will always have to do is specify a base image, it can be a known image you own, or hub.docker.com image, anywhere
'FROM node:15' #will take the 'node' image from docker hub the ':15' specifies the version number (15 here) default is ':latest'
'WORKDIR /app' #notice how all commands are in caps, this will set the working directory to be the '/app' directory within the container
  After setting the work directory, any time you run a command, it's going to run that command from that directory, so put all the application code in /app, then run 'index.js' on '/app' and it will run
'COPY package.json .' #'package.json' contains all dependancies, will copy it into the '.' is a relative directory it means our current directory which we set above with 'WORKDIR'(/app)
'RUN npm install' #will run a command from the working directory, since we copied the 'package.json' it will install all dependancies
'COPY . ./' #will copy all remaining files from folder into the working directory, optimization technique, to add this after the package.json - has to do with layering and builds, wrote about it below
Each command we ran above creates another layer, since docker is efficient, it will know that nothing has changed, and will rebuild only the layers that have changes when you 'docker build'
  so you won't need to run 'npm install' after each docker build because you seperated the layers above with the 2 COPY statements, if you change your code you will just rebuild the fifth layer'COPY . ./'
'EXPOSE 3000' #our app will be running on port 3000
'CMD ["node", "index.js"]' #will run 'node index.js' from terminal after being deployed in container at runtime

'docker build .' #from the terminal '.' is context for current working directory, which we are already in, we have named our 'Dockerfile' and it worked, even with old node version(15, current is 24?) cool
1/5: Grabs image from docker hub - 2/5 Sets working dir - 3/5 Copies package.json - 4/5 runs npm install - 5/5 copies remaining files - caches each result, so next time you run will be quicker
'docker image ls' #we forgot to name our image above, so it is labeled <NONE>, but still has an IMAGE ID
'docker image rm <IMAGE ID>' #will remove the docker image with the ID 
'docker build -t node-app-image .' #will create a new image with the '-t' flag will allow us to name it, '.' is context for current working directory 
'docker run node-app-image' #will run our newly created image
'docker run -d --name node-app node-app-image' #will run custom image, but with '-d' flag which is detached mode(will allow you to use CLI), and '--name' flag which will name it 'node-app'
'docker ps' will list all containers currently running at the moment

Container is running, but page won't load, to fix this you need to :
'EXPOSE 3000' #port is exposed, allegedly, this doesn't work though, it's a comment for the dockerfile to let other people know that the image expects others to open port 3000 to work, it doesn't actually do it
by default, as a security feature, docker does not allow anything from outside to communicate with it, it can communicate to the outside but no outside in
to change this, you need to tell the host machine if you receive traffic from a specific port - you want to forward that traffic to the docker container
'docker rm node-app -f' #will remove the current running container we just created, '-f' flag like other programs is force, normally you need to stop a container before you delete it, it won't delete image
'docker run -p 3000:3000 -d --name node-app node-app-image' #same command as above, but this time '-p' flag '3000:3000' 
  left:right- right number on the right is where traffic is sent on container(app is listening to 3000)
  left: represents traffic coming in from outside, so if another device on network/localhost is sending traffic to localhost:3000, it will send it to port 3000 on container
  For clarity, these 2 numbers are currently the same, but they don't have to be the same, 
  '4000:3000' if anyone sends anything to our localhost on port '4000', it will be forwarded the containers port 3000 - if that makes sense for future sean 
  right:left - right side is from outside to in, left side is from in to in (from locahost machine to the container) - if that makes sense 
  the left will also take traffic sent from localhost port '4000' in example above and redirect it to port '3000' on the container - so any traffic sent or received from port 4000 will go to the container
    which it will then receive on the left port(3000)

CONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                    NAMES
3e210f2ccaf4   node-app-image   "docker-entrypoint.sâ€¦"   11 seconds ago   Up 10 seconds   0.0.0.0:3000->3000/tcp   node-app
Above is the response from 'docker ps' which shows all running containers, we sent the command with '3000:3000' notice the 'PORTS' - 
  it will take any traffic sent/received from port 3000 and forward it to containers port 3000, here was the command : 'docker run -p 3000:3000 -d --name node-app node-app-image'
just tested, now everything works, we need to pass the '-p' flag in order to use it for express the 'EXPOSE' in the dockerfile is meant to tell others what ports to open for the app

'docker exec -it node-app bash' #will allow us to inspect with the '-it' flag (interactive mode) 'bash' will replace the 'index.js' and allow us to take a look at the app filesystem
root@3e210f2ccaf4:/app #above command drops us into the terminal of the '/app' folder because that is where the working directory was set
'exit' #type to exit out of the docker terminal
'ls' # lists all files in directory 'Dockerfile  index.js  node_modules  package-lock.json  package.json'
You don't actually need the docker file to run the docker container, it is just used to build the app 'COPY . ./' copies everything, often times this is bad practice(like .env.local files)
  also the 'node_modules' folder was copied, we do a 'COPY package.json .' and 'RUN npm install' command earlier, so there is no reason why we should currently have that in our container
'.dockerignore' #similar to '.gitignore' tells docker which files to ignore, simply type in whichever files you don't want copied over 'node_modules Dockerfile .dockerignore .git .gitignore' ignored
after rebuilding and reruning command, .dockerignore works, the node_modules is still there, but that is from the 'npm install'
EVERY time you change the code, you will NEED to rebuild for the changes to affect the container, stale code

volumes = allow you to have persitant data in your containers, but 
Bind Mount = a specific type of volume that will allow you to sync a folder in localhost machine with a folder in the docker container(symlink?)
  So every time you change the code, it will automatically sync the new changes to the docker container, so you don't need to rebuild before every run
'docker run -v <PathToFolderOnLocalMachine>:<PathToFolderOnContainer> -p 3000:3000 -d --name node-app node-app-image' #here we pass the '-v' flag which stands for volume,
  it will allow us to symlink a folder from our local machine <Path>: to a folder on the container <Path>, which will be the /app working directory which we set earlier
'docker run -v /mnt/sdb4/Code/CourseraGoogleFCC/FCC-DevOps-Docker-Node-Express-Redis-Mongo/Docker:/app -p 3000:3000 -d --name node-app node-app-image' #without placeholder variables
In VSCODE/Cursor - You can 'Right Click' the File in the 'File Explorer' and go to 'Copy Path' and it will copy the path without having to type out a 100+ character string OR do below(for mac or linux) :
'docker run -v $(pwd):/app -p 3000:3000 -d --name node-app node-app-image' #from the terminal when you are in the correct directory '$(pwd)

ERROR: 
docker run -v /mnt/sdb4/Code/CourseraGoogleFCC/FCC-DevOps-Docker-Node-Express-Redis-Mongo/Docker:/app -p 3000:3000 -d --name node-app node-app-image
466c31fdfe50204cda30632b662d4a9705816bc1f5743c2349165c9d3b7051b2
docker: Error response from daemon: Mounts denied: 
The path /mnt/sdb4/Code/CourseraGoogleFCC/FCC-DevOps-Docker-Node-Express-Redis-Mongo/Docker is not shared from the host and is not known to Docker.
You can configure shared paths from Docker -> Preferences... -> Resources -> File Sharing.
See https://docs.docker.com/ for more info.
- Attempted, to do this, would be more convenient if I am using docker for developing, but I need to give permissions to docker somehow, gonna just move on for now, can fix if I need to: future sean problem
FIX(did you try turning it on and off again?):
https://github.com/portainer/portainer/issues/1732
'sudo systemctl restart docker' #will restart docker
From Docker Desktop -> Right Click the icon on bottom left of screen -> Settings -> Resources -> File Sharing -> add the directory

Finally fixed it, now 'docker run -v $(pwd):/app -p 3000:3000 -d --name node-app node-app-image' works, changed 'index.js' and 
'docker exec -it node-app bash' 'cat index.js' #will allow you to check the 'index.js', changed it on localhost machine, but 'localhost:3000' isn't working, the problem is:
  you need to start the node process again, or use 'nodemon' 
'nodemon' will look at your code, and if any changes take place it will restart the node process automatically so the changes are updated in real time 
'npm install nodemon --save-dev' #will install nodemon as a dev dependancy to get it into the 'package.json', and we don't need it to run when it gets to actual production
  "scripts": {   "start":"node index.js",   "dev":"nodemon index.js"  }, #added this script to the 'package.json', which will only run 'nodemon' in dev mode, and revert back to 'index.js' for production
'docker build -t node-app-image .' #since you changed the 'package.json', you will need to rebuild image before changes take effect, since it changed it will rerun 'npm install'(step 3)
  we will also need to change the 'Dockerfile' since we won't be running 'CMD ["node", "index.js"]' it will now be 'CMD ["npm", "run", "dev"]'
'docker run -v $(pwd):/app -p 3000:3000 -d --name node-app node-app-image' #now run the newly built container, and test the changes
Just tested, now we can change the code on the fly, without having to rebuild every single time, pretty cool
'docker ps -a' #will show all containers, '-a' flag all (started or stopped) will show with date time started or stopped
'docker logs node-app' #will give you all logs for docker container 'node-app'
When you 'COPY . ./' which copies everything in the current directory you are in from the terminal, to the working directory of the container, it will override everything like the 'node_modules' folder
  so when you delete 'nodemon' or 'node_modules' from your local machine, it will also delete it from your container, You can get around this by creating another volume 
'Anonymous Volume' = will allow you to preserve the '/app/node_modules' with specificity, you can ensure that the bind mount doesn't override the folder within the '/app' directory
'docker run -v $(pwd):/app -v /app/node_modules -p 3000:3000 -d --name node-app node-app-image' #notice how there are 2 '-v' tags, it will now ignore the '/app/node_modules' overrides bind mount
  so now we can delete our 'node_modules' on the local machine, because it will be in the container, and save space, and won't be deleted by the 'COPY . ./' because of the new '-v' flag
'docker exec -it node-app bash' #to check the container, with the new bind mount, and the symlink updates -
  it works both ways, if you create a file in container it will update localhost, if you update localhost, it will update the container, to change this, make it a 
  'Read Only BindMount' = so changes in the container won't affect the local host, because that is a security issue
'docker run -v $(pwd):/app:ro -v /app/node_modules -p 3000:3000 -d --name node-app node-app-image' #notice the 'ro' after the first '/app' this will make it a 'ReadOnly BindMount'

How to use '.env' variables within a docker container:
'ENV PORT 3000' #sets the default value for port, in the 'Dockerfile'
'EXPOSE $PORT' #you can reference the above port variable, that will update the 'EXPOSE' if the 'ENV PORT 3000' get's changed
'docker build -t node-app-image .' #since we made a change to the 'Dockerfile' you need to rebuild
When you actually deploy the container, you can specify what 'ENV' variable you want to be set, the 'ENV PORT 3000' is just the default value, we can override it
'--env' or '-e' #you can use either of these for syntax to set an environment variable for the 'docker run ...' command
'docker run -v $(pwd):/app -v /app/node_modules --env PORT=4000 -p 3000:4000 -d --name node-app node-app-image' #will change the environment variable and container listening port to 'PORT=4000'
'docker exec -it node-app bash' #to check env variables, see if they were overwritten
  'printenv' #will allow us to check docker environment variables of our docker container in bash
Create a new file for environment variables '.env' add 'PORT=4000' - to load the environment variables, 
'docker run -v $(pwd):/app -v /app/node_modules --env-file ./.env -p 3000:4000 -d --name node-app node-app-image' #notice the '--env-file ./.env' which will set the environment variables from the file for container
'docker volume ls' #will show you all the volumes you've created, if you keep creating containers, they will get saved, every command like above that you run which is different, creates a new volume
  every time you delete your container, it will preserve the node_modules, etc folder, the entire idea of a volume is it's persistent data you want to preserve(like a postgres DB/etc)
  'docker volume rm <VOLUME NAME>' You can always go in and manually delete the volumes 
'docker volume prune' #will remove all unneccsasry volumes
'docker rm node-app -fv' #'-f' flag = force, notice the '-v' flag = volume, will also delete the volume that is associated with that container so they don't build up

The container we have currently is set up for good workflow for developing a Node/Express App, but we have a super long command to run it correctly, and for development/production we sometimes need multiple
  docker containers, all with their own very long command strings to run each one, to fix this we :
Docker Compose = A feature that will allow us to create a file that has all the steps and configuration settings for each docker container, with one command :


















